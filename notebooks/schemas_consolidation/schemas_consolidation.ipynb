{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "import chardet\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'API_URL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cg/b2_z5vjn52x7nyrtvtmwqsqh0000gp/T/ipykernel_80641/2627800266.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#API PARAMETERS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAPI_URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m HEADERS = {\n",
      "\u001b[0;31mNameError\u001b[0m: name 'API_URL' is not defined"
     ]
    }
   ],
   "source": [
    "#API PARAMETERS\n",
    "\n",
    "api_url = API_URL\n",
    "\n",
    "HEADERS = {\n",
    "    'X-API-KEY': API_KEY,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('.')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_path = Path(WORKING_DIR)\n",
    "current_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path = Path(TMP_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(current_path, 'config_tableschema.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas_list_url = SCHEMA_CATALOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_url_base = api_url + 'datasets/?schema={schema_name}'\n",
    "tag_url_base = api_url + 'datasets/?tag={tag}'\n",
    "search_url_base = api_url + 'datasets?q={search_word}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "validata_base_url = \"https://validata-api.app.etalab.studio/validate?schema={schema_url}&url={rurl}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMUM_VALID_RESOURCES_TO_CONSOLIDATE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the dictionnary with information on the schema (when schemas catagalogue list already loaded)\n",
    "def get_schema_dict(schema_name, schemas_catalogue_list) :\n",
    "    res = None\n",
    "    for schema in schemas_catalogue_list :\n",
    "        if schema['name'] == schema_name :\n",
    "            res = schema\n",
    "    \n",
    "    if res is None :\n",
    "        print(\"No schema named '{}' found.\".format(schema_name))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the schema default configuration in the configuration YAML file\n",
    "def add_schema_default_config(schema_name, config_path) :\n",
    "    \n",
    "    schema_dict = get_schema_dict(schema_name, schemas_catalogue_list)\n",
    "    schema_title = schema_dict['title']\n",
    "    \n",
    "    default_schema_config_dict = {\n",
    "        'consolidate': False,\n",
    "        'search_words':[schema_title] # setting schema title as a default search keyword for resources\n",
    "    }\n",
    "    \n",
    "    if os.path.exists(config_path) :\n",
    "        with open(config_path, 'r') as infile :\n",
    "            config_dict = yaml.safe_load(infile)\n",
    "    else :\n",
    "        config_dict = {}\n",
    "        \n",
    "    config_dict[schema_name] = default_schema_config_dict\n",
    "    \n",
    "    with open(config_path, 'w') as outfile:\n",
    "        yaml.dump(config_dict, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API parsing to get resources infos based on schema metadata, tags and search keywords\n",
    "def parse_api(url):\n",
    "    r = requests.get(url)\n",
    "    data = r.json()\n",
    "    nb_pages = int(data['total']/data['page_size'])+1\n",
    "    arr = []\n",
    "    for i in range(1,nb_pages+1):\n",
    "        r = requests.get(url+\"&page=\"+str(i))\n",
    "        data = r.json()\n",
    "        for dataset in data['data']:\n",
    "            for res in dataset['resources']:\n",
    "                if 'format=csv' in res['url']:\n",
    "                    filename = res['url'].split('/')[-3] + '.csv'\n",
    "                else:\n",
    "                    filename = res['url'].split('/')[-1]\n",
    "                ext = filename.split('.')[-1]\n",
    "                obj = {}\n",
    "                obj['dataset_id'] = dataset['id']\n",
    "                obj['dataset_title'] = dataset['title']\n",
    "                obj['dataset_slug'] = dataset['slug']\n",
    "                obj['dataset_page'] = dataset['page']\n",
    "                obj['resource_id'] = res['id']\n",
    "                obj['resource_title'] = res['title']\n",
    "                obj['resource_url'] = res['url']\n",
    "                obj['resource_last_modified'] = res['last_modified']\n",
    "                if ext != 'csv':\n",
    "                    obj['error_type'] = \"wrong-file-format\"\n",
    "                else:\n",
    "                    if not dataset['organization'] and not dataset['owner']:\n",
    "                        obj['error_type'] = \"orphan-dataset\"\n",
    "                    else:\n",
    "                        obj['organization_or_owner'] = dataset['organization']['slug'] if dataset['organization'] else dataset['owner']['slug']\n",
    "                        obj['error_type'] = None\n",
    "                arr.append(obj)\n",
    "    df = pd.DataFrame(arr)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the validation report based on the resource url, schema url and validation url\n",
    "def make_validata_report(rurl, schema_url, validata_base_url=validata_base_url) :\n",
    "    r = requests.get(validata_base_url.format(schema_url=schema_url, rurl=rurl))\n",
    "    time.sleep(0.5)\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns if a resource is valid or not regarding a schema (version)\n",
    "def is_validata_valid(rurl, schema_url, validata_base_url=validata_base_url) :\n",
    "    try :\n",
    "        report = make_validata_report(rurl, schema_url, validata_base_url=validata_base_url)\n",
    "        try :\n",
    "            res = report['report']['valid']\n",
    "        except:\n",
    "            print('{} ---- 🔴 No info in validata report for resource: {}'.format(datetime.today(), rurl))\n",
    "            res = False\n",
    "            report = None\n",
    "    except JSONDecodeError :\n",
    "        print('{} ---- 🔴 Could not make JSON from validata report for resource: {}'.format(datetime.today(), rurl))\n",
    "        res = False\n",
    "        report = None\n",
    "    return res, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_validata_report(res, report, version, schema_name, dataset_id, resource_id):\n",
    "    save_report = {}\n",
    "    save_report['validation-report:schema_name'] = schema_name\n",
    "    save_report['validation-report:schema_version'] = version\n",
    "    save_report['validation-report:schema_type'] = 'tableschema'\n",
    "    save_report['validation-report:validator'] = 'validata'\n",
    "    save_report['validation-report:valid_resource'] = res\n",
    "    try:\n",
    "        nb_errors = report['report']['stats']['errors'] if report['report']['stats']['errors'] < 100 else 100\n",
    "    except:\n",
    "        nb_errors = None\n",
    "    save_report['validation-report:nb_errors'] = nb_errors\n",
    "    try:\n",
    "        keys = 'cells'\n",
    "        errors = [ { y: x[y]  for y in x if y not in keys } for x in report['report']['tasks'][0]['errors'][:100]]\n",
    "\n",
    "    except:\n",
    "        errors = None\n",
    "    save_report['validation-report:errors'] = errors\n",
    "    save_report['validation-report:validation_date'] = str(datetime.now())\n",
    "    with open(str(validata_reports_path) + '/' + dataset_id + '_'+resource_id + '_' + version + '.json', 'w') as f:\n",
    "        json.dump(save_report, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns if a resource is valid based on its \"ref_table\" row\n",
    "def is_validata_valid_row(row, schema_url, version, schema_name) :\n",
    "    if row['error_type'] is None : #if no error\n",
    "        rurl = row['resource_url']\n",
    "        res, report = is_validata_valid(rurl, schema_url)\n",
    "        if report: save_validata_report(res, report, version, schema_name, row['dataset_id'], row['resource_id'])\n",
    "        return res\n",
    "    else :\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on validation columns by version, adds a column to the ref_table that shows the most recent version of the schema for which the resource is valid\n",
    "def add_most_recent_valid_version(df_ref) :\n",
    "    \n",
    "    version_cols_list = [col for col in df_ref.columns if col.startswith('is_valid_v_')]\n",
    "    \n",
    "    df_ref['most_recent_valid_version'] = ''\n",
    "    \n",
    "    for col in sorted(version_cols_list, reverse=True) :\n",
    "        df_ref.loc[(df_ref['most_recent_valid_version'] == ''), 'most_recent_valid_version'] = df_ref.loc[(df_ref['most_recent_valid_version'] == ''), col].apply(lambda x : x*col.replace('is_valid_v_',''))\n",
    "    \n",
    "    df_ref.loc[(df_ref['most_recent_valid_version'] == ''), 'most_recent_valid_version'] = np.nan\n",
    "    \n",
    "    return df_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the current metadata of schema version of a resource (based of ref_table row)\n",
    "def get_resource_schema_version(row, api_url) :\n",
    "    \n",
    "    dataset_id = row['dataset_id']\n",
    "    resource_id = row['resource_id']\n",
    "    \n",
    "    url = api_url + 'datasets/{}/resources/{}/'.format(dataset_id, resource_id)\n",
    "    r = requests.get(url)\n",
    "    if r.status_code == 200 :\n",
    "        r_json = r.json()\n",
    "        if 'schema' in r_json.keys() :\n",
    "            if 'version' in r_json['schema'].keys() :\n",
    "                return r_json['schema']['version']\n",
    "            else :\n",
    "                return np.nan\n",
    "        else :\n",
    "            return np.nan\n",
    "    else :\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220218\n"
     ]
    }
   ],
   "source": [
    "consolidation_date_str = DATE_AIRFLOW\n",
    "print(consolidation_date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = tmp_path / 'data'\n",
    "data_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_data_path = tmp_path / 'consolidated_data'\n",
    "consolidated_data_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_tables_path = tmp_path / 'ref_tables'\n",
    "ref_tables_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_tables_path = tmp_path / 'report_tables'\n",
    "report_tables_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "validata_reports_path = tmp_path / 'validata_reports'\n",
    "validata_reports_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = tmp_path / 'output'\n",
    "output_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading schemas documentation and config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep **only table schemas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema catalogue URL: https://opendataschema.frama.io/catalog/schema-catalog.json\n",
      "Version: 1\n",
      "Total number of schemas: 2\n",
      "- etalab/schema-lieux-covoiturage (8 versions)\n",
      "- NaturalSolutions/schema-arbre (4 versions)\n"
     ]
    }
   ],
   "source": [
    "# Getting schemas list :\n",
    "\n",
    "#Keeping track of schema info\n",
    "schemas_report_dict = {}\n",
    "\n",
    "schemas_catalogue_dict = requests.get(schemas_list_url).json()\n",
    "print('Schema catalogue URL: {}'.format(schemas_catalogue_dict['$schema']))\n",
    "print('Version: {}'.format(schemas_catalogue_dict['version']))\n",
    "\n",
    "schemas_catalogue_list = [schema for schema in schemas_catalogue_dict['schemas'] if schema['schema_type'] == 'tableschema']\n",
    "nb_schemas = len(schemas_catalogue_list)\n",
    "print('Total number of schemas: {}'.format(nb_schemas))\n",
    "\n",
    "for schema in schemas_catalogue_list :\n",
    "    print('- {} ({} versions)'.format(schema['name'], len(schema['versions'])))\n",
    "    schemas_report_dict[schema['name']] = {'nb_versions':len(schema['versions'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating/updating config file with missing schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-18 15:55:20.103361 - 🆗 Schema etalab/schema-lieux-covoiturage already in config file.\n",
      "2022-02-18 15:55:20.103471 - 🆗 Schema NaturalSolutions/schema-arbre already in config file.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(config_path) :\n",
    "    with open(config_path, 'r') as f :\n",
    "        config_dict = yaml.safe_load(f)\n",
    "else :\n",
    "    config_dict = {}\n",
    "    \n",
    "for schema in schemas_catalogue_list :\n",
    "    if schema['name'] not in config_dict.keys() :\n",
    "        add_schema_default_config(schema['name'], config_path)\n",
    "        schemas_report_dict[schema['name']]['new_config_created'] = True\n",
    "        print('{} - ➕ Schema {} added to config file.'.format(datetime.today(), schema['name']))\n",
    "    else :\n",
    "        schemas_report_dict[schema['name']]['new_config_created'] = False\n",
    "        print('{} - 🆗 Schema {} already in config file.'.format(datetime.today(), schema['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**⚠️⚠️⚠️ EDIT CONFIG FILE IF NEEDED (especially for new schemas) ⚠️⚠️⚠️**\n",
    "\n",
    "Then, reload config file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_path, 'r') as f :\n",
    "    config_dict = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building reference tables (parsing and listing resources + Validata check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-18 15:55:20.769294 - ℹ️ STARTING SCHEMA: NaturalSolutions/schema-arbre\n",
      "2022-02-18 15:55:20.889225 -- ⚠️ No resource found for this schema.\n",
      "2022-02-18 15:55:20.889357 - ℹ️ STARTING SCHEMA: etalab/schema-lieux-covoiturage\n",
      "2022-02-18 15:55:21.254242 -- 🔢 4 resource(s) found for this schema.\n",
      "2022-02-18 15:55:25.106483 --- ☑️ Validata check done for version 0.0.1\n",
      "2022-02-18 15:55:28.518031 --- ☑️ Validata check done for version 0.1.0\n",
      "2022-02-18 15:55:31.896324 --- ☑️ Validata check done for version 0.1.1\n",
      "2022-02-18 15:55:35.373706 --- ☑️ Validata check done for version 0.1.2\n",
      "2022-02-18 15:55:39.111804 --- ☑️ Validata check done for version 0.2.0\n",
      "2022-02-18 15:55:42.879260 --- ☑️ Validata check done for version 0.2.1\n",
      "2022-02-18 15:55:46.638876 --- ☑️ Validata check done for version 0.2.2\n",
      "2022-02-18 15:55:50.391242 --- ☑️ Validata check done for version 0.2.3\n",
      "2022-02-18 15:55:50.413884 -- ✅ Validata check done for etalab/schema-lieux-covoiturage.\n",
      "CPU times: user 948 ms, sys: 104 ms, total: 1.05 s\n",
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for schema_name in config_dict.keys() :\n",
    "    \n",
    "    print('{} - ℹ️ STARTING SCHEMA: {}'.format(datetime.now(), schema_name))\n",
    "    \n",
    "    #NEEDED PARAMETERS\n",
    "    \n",
    "    #Schema description and consolidation configuration    \n",
    "    schema_config = config_dict[schema_name]\n",
    "    \n",
    "    if schema_config['consolidate'] == True :\n",
    "    \n",
    "        #Schema official specification (in catalogue)\n",
    "        schema_dict = get_schema_dict(schema_name, schemas_catalogue_list)\n",
    "\n",
    "        #Datasets to exclude (from config)\n",
    "        datasets_to_exclude = []\n",
    "        if 'consolidated_dataset_id' in schema_config.keys() :\n",
    "            datasets_to_exclude += [schema_config['consolidated_dataset_id']]\n",
    "        if 'exclude_dataset_ids' in schema_config.keys() :\n",
    "            if type(schema_config['exclude_dataset_ids']) == list :\n",
    "                datasets_to_exclude += schema_config['exclude_dataset_ids']\n",
    "\n",
    "        #Tags and search words to use to get resources that could match schema (from config)\n",
    "        tags_list = []\n",
    "        if 'tags' in schema_config.keys() :\n",
    "            tags_list += schema_config['tags']\n",
    "\n",
    "        search_words_list = []\n",
    "        if 'search_words' in schema_config.keys() :\n",
    "            search_words_list = schema_config['search_words']\n",
    "\n",
    "        #Schema versions not to consolidate\n",
    "        drop_versions = []\n",
    "        if 'drop_versions' in schema_config.keys() :\n",
    "            drop_versions += schema_config['drop_versions']\n",
    "        \n",
    "        schemas_report_dict[schema_name]['nb_versions_to_drop_in_config'] = len(drop_versions)\n",
    "\n",
    "        #PARSING API TO GET ALL ELIGIBLE RESOURCES FOR CONSOLIDATION\n",
    "\n",
    "        df_list = []\n",
    "\n",
    "        #Listing resources by schema request\n",
    "        df_schema = parse_api(schema_url_base.format(schema_name=schema_name))\n",
    "        schemas_report_dict[schema_name]['nb_resources_found_by_schema'] = len(df_schema)\n",
    "        if len(df_schema) > 0 :\n",
    "            df_schema['resource_found_by'] = '1 - schema request'\n",
    "            df_schema['initial_version_name'] = df_schema.apply(lambda row : get_resource_schema_version(row, api_url), axis=1)\n",
    "            df_list += [df_schema]\n",
    "\n",
    "        #Listing resources by tag requests\n",
    "        schemas_report_dict[schema_name]['nb_resources_found_by_tags'] = 0\n",
    "        for tag in tags_list : \n",
    "            df_tag = parse_api(tag_url_base.format(tag=tag))\n",
    "            schemas_report_dict[schema_name]['nb_resources_found_by_tags'] += len(df_tag)\n",
    "            if len(df_tag) > 0 :\n",
    "                df_tag['resource_found_by'] = '2 - tag request'\n",
    "                df_list += [df_tag]\n",
    "\n",
    "        #Listing resources by search (keywords) requests\n",
    "        schemas_report_dict[schema_name]['nb_resources_found_by_search_words'] = 0\n",
    "        for search_word in search_words_list :\n",
    "            df_search_word = parse_api(search_url_base.format(search_word=search_word))\n",
    "            schemas_report_dict[schema_name]['nb_resources_found_by_search_words'] += len(df_search_word)\n",
    "            if len(df_search_word) > 0 :\n",
    "                df_search_word['resource_found_by'] = '3 - search request'\n",
    "                df_list += [df_search_word]\n",
    "\n",
    "        if len(df_list) > 0 :\n",
    "            df = pd.concat(df_list, ignore_index=True)\n",
    "            df = df[~(df['dataset_id'].isin(datasets_to_exclude))]\n",
    "            df = df.sort_values('resource_found_by')\n",
    "            df = df.drop_duplicates(subset=['resource_id'], keep='first')\n",
    "            \n",
    "            print('{} -- 🔢 {} resource(s) found for this schema.'.format(datetime.now(), len(df)))\n",
    "            \n",
    "            if 'initial_version_name' not in df.columns : # in case there is no resource found by schema request\n",
    "                df['initial_version_name'] = np.nan\n",
    "\n",
    "            #FOR EACH RESOURCE AND SCHEMA VERSION, CHECK IF RESOURCE MATCHES THE SCHEMA VERSION\n",
    "\n",
    "            #Apply validata check for each version that is not explicitly dropped in config file\n",
    "            version_names_list = []\n",
    "            \n",
    "            for version in schema_dict['versions'] :\n",
    "                version_name = version['version_name']\n",
    "                if version_name not in drop_versions :\n",
    "                    schema_url = version['schema_url']\n",
    "                    df['is_valid_v_{}'.format(version_name)] = df.apply(lambda row : is_validata_valid_row(row, schema_url, version_name, schema_name), axis=1)\n",
    "                    version_names_list += [version_name]\n",
    "                    print('{} --- ☑️ Validata check done for version {}'.format(datetime.now(), version_name))\n",
    "                else :\n",
    "                    print('{} --- ❌ Version {} to drop according to config file'.format(datetime.now(), version_name))\n",
    "            \n",
    "            if len(version_names_list) > 0 :\n",
    "                #Check if resources are at least matching one schema version (only those matching will be downloaded in next step)\n",
    "                df['is_valid_one_version'] = sum([df['is_valid_v_{}'.format(version_name)] for version_name in version_names_list]) > 0\n",
    "                schemas_report_dict[schema_name]['nb_valid_resources'] = df['is_valid_one_version'].sum()\n",
    "                df = add_most_recent_valid_version(df)\n",
    "                df.to_csv(os.path.join(ref_tables_path, 'ref_table_{}.csv'.format(schema_name.replace('/', '_'))), index=False)\n",
    "                print('{} -- ✅ Validata check done for {}.'.format(datetime.now(), schema_name))\n",
    "            else :\n",
    "                schemas_report_dict[schema_name]['nb_valid_resources'] = 0\n",
    "                print('{} -- ❌ All possible versions for this schema were dropped by config file.'.format(datetime.now()))\n",
    "\n",
    "        else :\n",
    "            print('{} -- ⚠️ No resource found for this schema.'.format(datetime.now(), schema_name))\n",
    "            \n",
    "    else :\n",
    "        print('{} -- ❌ Schema not to consolidate according to config file.'.format(datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading valid data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download only data that is valid for at least one version of the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-18 15:55:50.439325 - ℹ️ STARTING SCHEMA: NaturalSolutions/schema-arbre\n",
      "2022-02-18 15:55:50.439786 -- ❌ No reference table made for this schema (schema not to consolidate, no version to consolidate or no resource found).\n",
      "2022-02-18 15:55:50.439820 - ℹ️ STARTING SCHEMA: etalab/schema-lieux-covoiturage\n",
      "2022-02-18 15:55:50.485844 --- ⬇️✅ downloaded file [bnlc.csv] https://raw.githubusercontent.com/etalab/transport-base-nationale-covoiturage/main/bnlc-.csv\n",
      "2022-02-18 15:55:50.536462 --- ⬇️✅ downloaded file [n_lieux_covoit_p_035] https://static.data.gouv.fr/resources/lieux-de-covoiturage-departement-dille-et-vilaine/20210722-131413/data.csv\n",
      "CPU times: user 37.9 ms, sys: 6.3 ms, total: 44.2 ms\n",
      "Wall time: 98.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for schema_name in config_dict.keys() :\n",
    "    \n",
    "    print('{} - ℹ️ STARTING SCHEMA: {}'.format(datetime.now(), schema_name))\n",
    "    \n",
    "    ref_table_path = os.path.join(ref_tables_path, 'ref_table_{}.csv'.format(schema_name.replace('/', '_')))\n",
    "    \n",
    "    if os.path.exists(ref_table_path) :\n",
    "        df_ref = pd.read_csv(ref_table_path)\n",
    "        df_ref['is_downloaded'] = False\n",
    "        \n",
    "        if len(df_ref[df_ref['is_valid_one_version'] == True]) > 0 :\n",
    "        \n",
    "            schema_data_path = Path(data_path) / schema_name.replace('/','_')\n",
    "            schema_data_path.mkdir(exist_ok=True)\n",
    "\n",
    "            for index,row in df_ref[df_ref['is_valid_one_version'] == True].iterrows():\n",
    "                rurl = row['resource_url']\n",
    "                r = requests.get(rurl, allow_redirects=True)\n",
    "                \n",
    "                if r.status_code == 200 :\n",
    "                    p = Path(schema_data_path) / row['dataset_slug']\n",
    "                    p.mkdir(exist_ok=True)\n",
    "                    written_filename = '{}.csv'.format(row['resource_id'])\n",
    "\n",
    "                    with open('{}/{}'.format(p, written_filename), 'wb') as f:\n",
    "                        f.write(r.content)\n",
    "                    \n",
    "                    df_ref.loc[(df_ref['resource_id'] == row['resource_id']), 'is_downloaded'] = True\n",
    "                    \n",
    "                    print('{} --- ⬇️✅ downloaded file [{}] {}'.format(datetime.now(), row['resource_title'], rurl))\n",
    "                else :\n",
    "                    print('{} --- ⬇️❌ File could not be downloaded: [{}] {}'.format(datetime.now(), row['resource_title'], rurl))\n",
    "                    \n",
    "        else :\n",
    "            print('{} -- ⚠️ No valid resource for this schema'.format(datetime.now()))\n",
    "            \n",
    "        df_ref.to_csv(ref_table_path, index=False)\n",
    "    \n",
    "    else :\n",
    "        print('{} -- ❌ No reference table made for this schema (schema not to consolidate, no version to consolidate or no resource found).'.format(datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-18 15:55:50.563986 - ℹ️ STARTING SCHEMA: NaturalSolutions/schema-arbre\n",
      "2022-02-18 15:55:50.564437 -- ❌ No data downloaded for this schema.\n",
      "2022-02-18 15:55:50.564481 - ℹ️ STARTING SCHEMA: etalab/schema-lieux-covoiturage\n",
      "2022-02-18 15:55:50.708703 -- ⚠️ Less than 5 (non-empty) valid resources for version 0.0.1 : consolidation file is not built\n",
      "2022-02-18 15:55:50.835791 -- ⚠️ Less than 5 (non-empty) valid resources for version 0.1.0 : consolidation file is not built\n",
      "2022-02-18 15:55:50.960851 -- ⚠️ Less than 5 (non-empty) valid resources for version 0.1.1 : consolidation file is not built\n",
      "2022-02-18 15:55:51.077539 -- ⚠️ Less than 5 (non-empty) valid resources for version 0.1.2 : consolidation file is not built\n",
      "2022-02-18 15:55:55.546109 -- ✅ DONE: etalab/schema-lieux-covoiturage version 0.2.0\n",
      "2022-02-18 15:56:00.251071 -- ✅ DONE: etalab/schema-lieux-covoiturage version 0.2.1\n",
      "2022-02-18 15:56:04.684648 -- ✅ DONE: etalab/schema-lieux-covoiturage version 0.2.2\n",
      "2022-02-18 15:56:09.182769 -- ✅ DONE: etalab/schema-lieux-covoiturage version 0.2.3\n",
      "CPU times: user 18.3 s, sys: 116 ms, total: 18.4 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for schema_name in config_dict.keys() :\n",
    "    \n",
    "    print('{} - ℹ️ STARTING SCHEMA: {}'.format(datetime.now(), schema_name))\n",
    "    \n",
    "    schema_data_path = Path(data_path) / schema_name.replace('/','_')\n",
    "    \n",
    "    if os.path.exists(schema_data_path) :\n",
    "        \n",
    "        schema_consolidated_data_path = Path(consolidated_data_path) / schema_name.replace('/','_')\n",
    "        schema_consolidated_data_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        ref_table_path = os.path.join(ref_tables_path, 'ref_table_{}.csv'.format(schema_name.replace('/', '_')))\n",
    "        df_ref = pd.read_csv(ref_table_path) #(This file necessarily exists if data folder exists)\n",
    "        \n",
    "        #We will test if downloaded files are empty or not (so we set default values)\n",
    "        df_ref['is_empty'] = np.nan\n",
    "        df_ref.loc[(df_ref['is_downloaded'] == True), 'is_empty'] = False\n",
    "        \n",
    "        schema_dict = get_schema_dict(schema_name, schemas_catalogue_list)\n",
    "        \n",
    "        version_names_list = [col.replace('is_valid_v_','') for col in df_ref.columns if col.startswith('is_valid_v_')]\n",
    "        \n",
    "        for version in schema_dict['versions'] :\n",
    "            version_name = version['version_name']\n",
    "            if version_name in version_names_list :\n",
    "                df_ref_v = df_ref[(df_ref['is_valid_v_'+version_name] == True) & (df_ref['is_downloaded'] == True)]\n",
    "                \n",
    "                if len(df_ref_v) > 0 :\n",
    "                    #Get schema version parameters for ddup\n",
    "                    version_dict = requests.get(version['schema_url']).json()\n",
    "                    version_cols_list = [field_dict['name'] for field_dict in version_dict['fields']]\n",
    "                    \n",
    "                    if 'primaryKey' in version_dict.keys() :\n",
    "                        primary_key = version_dict['primaryKey']\n",
    "                    else :\n",
    "                        primary_key = None\n",
    "                        \n",
    "                    df_r_list = []\n",
    "                    \n",
    "                    for index,row in df_ref_v.iterrows():\n",
    "                        file_path = os.path.join(schema_data_path, row['dataset_slug'], '{}.csv'.format(row['resource_id']))\n",
    "                        with open(file_path,'rb') as f:\n",
    "                            encoding = chardet.detect(f.read()).get('encoding')\n",
    "                            if(encoding == 'Windows-1254'):\n",
    "                                encoding = 'iso-8859-1'\n",
    "                        \n",
    "                        df_r = pd.read_csv(file_path, sep=None, engine=\"python\", dtype='str', encoding=encoding, na_filter=False)\n",
    "                        \n",
    "                        if len(df_r) > 0 : #Keeping only non empty files\n",
    "                            #Keep only schema columns (and add empty columns for missing ones)\n",
    "                            df_r = df_r[[col for col in version_cols_list if col in df_r.columns]]\n",
    "                            for col in version_cols_list :\n",
    "                                if col not in df_r.columns :\n",
    "                                    df_r[col] = np.nan\n",
    "\n",
    "                            df_r['last_modified'] = row['resource_last_modified']\n",
    "                            df_r['datagouv_dataset_id'] = row['dataset_id']\n",
    "                            df_r['datagouv_resource_id'] = row['resource_id']\n",
    "                            df_r['datagouv_organization_or_owner'] = row['organization_or_owner']\n",
    "                            df_r_list += [df_r]\n",
    "                            \n",
    "                        else :\n",
    "                            df_ref.loc[(df_ref['resource_id'] == row['resource_id']), 'is_empty'] = True\n",
    "                    \n",
    "                    if len(df_r_list) >= MINIMUM_VALID_RESOURCES_TO_CONSOLIDATE :\n",
    "                        df_conso = pd.concat(df_r_list, ignore_index=True)\n",
    "\n",
    "                        #Sorting by most recent (resource last modification date at the moment)\n",
    "                        df_conso = df_conso.sort_values('last_modified', ascending=False)\n",
    "\n",
    "                        #Deduplication\n",
    "                        if primary_key is not None :\n",
    "                            ddup_cols = primary_key\n",
    "                        else :\n",
    "                            ddup_cols = version_cols_list\n",
    "\n",
    "                        df_conso = df_conso.drop_duplicates(ddup_cols, keep='first').reset_index(drop=True)  \n",
    "                        \n",
    "                        #Avoid \"null\" value in CSV files\n",
    "                        df_conso = df_conso.fillna('')\n",
    "                        \n",
    "                        df_conso.to_csv(os.path.join(schema_consolidated_data_path, 'consolidation_{}_v_{}_{}.csv'.format(schema_name.replace('/','_'), version_name, consolidation_date_str)), index=False, encoding=\"utf-8\",na_rep='null')\n",
    "                        print('{} -- ✅ DONE: {} version {}'.format(datetime.today(), schema_name, version_name))\n",
    "                    \n",
    "                    else :\n",
    "                        print('{} -- ⚠️ Less than 5 (non-empty) valid resources for version {} : consolidation file is not built'.format(datetime.today(), version_name))\n",
    "                    \n",
    "                else :\n",
    "                    print('{} -- ⚠️ No valid resource for version {} of this schema'.format(datetime.today(), version_name))\n",
    "        \n",
    "        df_ref.to_csv(ref_table_path, index=False)\n",
    "    \n",
    "    else :\n",
    "        print('{} -- ❌ No data downloaded for this schema.'.format(datetime.today()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"schemas_report_dict.pickle\",\"wb\") as f:\n",
    "    pickle.dump(schemas_report_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19ece5f74f8baf7d074e990d308e7c75b7ac8b98c7c6e76faedd0c3526d006f2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('analytics': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
