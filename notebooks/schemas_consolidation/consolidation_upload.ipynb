{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import requests\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "import emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API PARAMETERS\n",
    "\n",
    "api_url = API_URL\n",
    "\n",
    "HEADERS = {\n",
    "    'X-API-KEY': API_KEY,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIL PARAMETERS\n",
    "# smtp_host = os.environ['SCHEMA_BOT_MAIL_SMTP']\n",
    "# smtp_user = os.environ['SCHEMA_BOT_MAIL_USER']\n",
    "# smtp_password = os.environ['SCHEMA_BOT_MAIL_PASSWORD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path = Path(TMP_FOLDER)\n",
    "\n",
    "consolidated_data_path = tmp_path / 'consolidated_data'\n",
    "consolidated_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ref_tables_path = tmp_path / 'ref_tables'\n",
    "ref_tables_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "report_tables_path = tmp_path / 'report_tables'\n",
    "report_tables_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "validata_reports_path = tmp_path / 'validata_reports'\n",
    "validata_reports_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "current_path = Path(WORKING_DIR)\n",
    "current_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(current_path, 'config_tableschema.yml')\n",
    "\n",
    "with open(config_path, 'r') as f :\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "with open(tmp_path / 'schemas_report_dict.pickle', 'rb') as f:\n",
    "     schemas_report_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidation_date_str = DATE_AIRFLOW\n",
    "print(consolidation_date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas_list_url = SCHEMA_CATALOG\n",
    "schemas_catalogue_dict = requests.get(schemas_list_url).json()\n",
    "schemas_catalogue_list = [schema for schema in schemas_catalogue_dict['schemas'] if schema['schema_type'] == 'tableschema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Template for consolidation datasets title\n",
    "\n",
    "datasets_title_template = 'Fichiers consolidés des données respectant le schéma \"{schema_title}\"'\n",
    "\n",
    "#Template for consolidation datasets description (Markdown)\n",
    "\n",
    "datasets_description_template = '''\n",
    "Ceci est un jeu de données généré automatiquement par Etalab. Il regroupe les données qui respectent le schéma {schema_name}, par version du schéma.\n",
    "\n",
    "La fiche présentant le schéma et ses caractéristiques est disponible sur [https://schema.data.gouv.fr/{schema_name}/latest.html](https://schema.data.gouv.fr/{schema_name}/latest.html)\n",
    "\n",
    "### Qu'est-ce qu'un schéma ?\n",
    "\n",
    "Les schémas de données permettent de décrire des modèles de données : quels sont les différents champs, comment sont représentées les données, quelles sont les valeurs possibles, etc.\n",
    "\n",
    "Vous pouvez retrouver l'ensemble des schémas au référentiel sur le site schema.data.gouv.fr\n",
    "\n",
    "### Comment sont produites ces données ?\n",
    "\n",
    "Ces données sont produites à partir des ressources publiées sur le site [data.gouv.fr](http://data.gouv.fr) par différents producteurs. Etalab détecte automatiquement les ressources qui obéissent à un schéma et concatène l'ensemble des données en un seul fichier, par version de schéma.\n",
    "\n",
    "Ces fichiers consolidés permettent aux réutilisateurs de manipuler un seul fichier plutôt qu'une multitude de ressources et contribue ainsi à améliorer la qualité de l'open data.\n",
    "\n",
    "### Comment intégrer mes données dans ces fichiers consolidés ?\n",
    "\n",
    "Si vous êtes producteurs de données et que vous ne retrouvez pas vos données dans ces fichiers consolidés, c'est probablement parce que votre ressource sur [data.gouv.fr](http://data.gouv.fr) n'est pas conforme au schéma. Vous pouvez vérifier la conformité de votre ressource via l'outil [https://publier.etalab.studio/upload?schema={schema_name}](https://publier.etalab.studio/upload?schema={schema_name})\n",
    "\n",
    "En cas de problème persistant, vous pouvez contacter le support data.gouv [lien vers [https://support.data.gouv.fr/](https://support.data.gouv.fr/)].\n",
    "\n",
    "### Comment produire des données conformes ?\n",
    "\n",
    "Un certain nombre d'outils existent pour accompagner les producteurs de données. Vous pouvez notamment vous connecter sur le site [https://publier.etalab.studio/select?schema={schema_name}](https://publier.etalab.studio/select?schema={schema_name}) pour pouvoir saisir vos données selon trois modes :\n",
    "\n",
    "- upload de fichier existant\n",
    "- saisie via formulaire\n",
    "- saisie via tableur\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Template for mail/comment (added, updated and deleted schema)\n",
    "\n",
    "added_schema_comment_template = '''\n",
    "Bonjour,\n",
    "\n",
    "Vous recevez ce message car suite à un contrôle automatique de vos données par notre robot de validation, nous constatons que le fichier {resource_title} de ce jeu de données est conforme au schéma {schema_name} (version {most_recent_valid_version}).\n",
    "Nous avons donc automatiquement ajouté à ce fichier la métadonnée de schéma correspondante, ce qui atteste de la qualité des données que vous avez publiées.\n",
    "\n",
    "Une question ? Écrivez à validation@data.gouv.fr en incluant l'URL du jeu de données concerné.\n",
    "'''\n",
    "\n",
    "updated_schema_comment_template = '''\n",
    "Bonjour,\n",
    "\n",
    "Vous recevez ce message car suite à un contrôle automatique de vos données par notre robot de validation, nous constatons que le fichier {resource_title} de ce jeu de données (qui respecte le schéma {schema_name}) n'avait pas dans ses métadonnées la version de schéma la plus récente qu'il respecte.\n",
    "Nous avons donc automatiquement mis à jour les métadonnées du fichier en indiquant la version adéquate du schéma.\n",
    "\n",
    "Version précédemment indiquée : {initial_version_name}\n",
    "Version mise à jour : {most_recent_valid_version}\n",
    "\n",
    "Une question ? Écrivez à validation@data.gouv.fr en incluant l'URL du jeu de données concerné.\n",
    "'''\n",
    "\n",
    "deleted_schema_mail_template_org = '''\n",
    "Bonjour,<br />\n",
    "<br />\n",
    "Vous recevez ce message automatique car vous êtes admin de l'organisation {organisation_name} sur data.gouv.fr. Votre organisation a publié le jeu de données {dataset_title}, dont le fichier {resource_title} se veut conforme au schéma {schema_name}.<br />\n",
    "Cependant, suite à un contrôle automatique de vos données par notre robot de validation, il s'avère que ce fichier ne respecte aucune version de ce schéma.<br />\n",
    "Nous avons donc automatiquement supprimé la métadonnée de schéma associée à ce fichier.<br />\n",
    "<br />\n",
    "Vous pouvez consulter le [rapport de validation](https://validata.etalab.studio/table-schema?input=url&schema_url={schema_url}&url={resource_url}&repair=true) pour vous aider à corriger les erreurs (ce rapport est relatif à la version la plus récente du schéma, mais votre fichier a bien été testé vis-à-vis de toutes les versions possibles du schéma).<br />\n",
    "<br />\n",
    "Vous pourrez alors restaurer la métadonnée de schéma une fois un fichier valide publié.<br />\n",
    "<br />\n",
    "Une question ? Écrivez à validation@data.gouv.fr en incluant l'URL du jeu de données concerné.<br />\n",
    "<br />\n",
    "Cordialement,<br />\n",
    "<br />\n",
    "L'équipe de data.gouv.fr\n",
    "'''\n",
    "\n",
    "deleted_schema_mail_template_own = '''\n",
    "Bonjour,<br />\n",
    "<br />\n",
    "Vous recevez ce message automatique car vous avez publié sur data.gouv.fr le jeu de données {dataset_title}, dont le fichier {resource_title} se veut conforme au schéma {schema_name}.<br />\n",
    "Cependant, suite à un contrôle automatique de vos données par notre robot de validation, il s'avère que ce fichier ne respecte aucune version de ce schéma.<br />\n",
    "Nous avons donc automatiquement supprimé la métadonnée de schéma associée à ce fichier.<br />\n",
    "<br />\n",
    "Vous pouvez consulter le [rapport de validation](https://validata.etalab.studio/table-schema?input=url&schema_url={schema_url}&url={resource_url}&repair=true) pour vous aider à corriger les erreurs (ce rapport est relatif à la version la plus récente du schéma, mais votre fichier a bien été testé vis-à-vis de toutes les versions possibles du schéma).<br />\n",
    "<br />\n",
    "Vous pourrez alors restaurer la métadonnée de schéma une fois un fichier valide publié.<br />\n",
    "<br />\n",
    "Une question ? Écrivez à validation@data.gouv.fr en incluant l'URL du jeu de données concerné.<br />\n",
    "<br />\n",
    "Cordialement,<br />\n",
    "<br />\n",
    "L'équipe de data.gouv.fr\n",
    "'''\n",
    "\n",
    "deleted_schema_comment_template = '''\n",
    "Bonjour,\n",
    "\n",
    "Vous recevez ce message car suite à un contrôle automatique de vos données par notre robot de validation, nous constatons que le fichier {resource_title} de ce jeu de données se veut conforme au schéma {schema_name} alors qu'il ne respecte aucune version de ce schéma.\n",
    "Nous avons donc automatiquement supprimé la métadonnée de schéma associée à ce fichier.\n",
    "\n",
    "Vous pouvez consulter le [rapport de validation](https://validata.etalab.studio/table-schema?input=url&schema_url={schema_url}&url={resource_url}&repair=true) pour vous aider à corriger les erreurs (ce rapport est relatif à la version la plus récente du schéma, mais votre fichier a bien été testé vis-à-vis de toutes les versions possibles du schéma).\n",
    "\n",
    "Vous pourrez alors restaurer la métadonnée de schéma une fois un fichier valide publié.\n",
    "\n",
    "Une question ? Écrivez à validation@data.gouv.fr en incluant l'URL du jeu de données concerné.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate utility functions\n",
    "\n",
    "#Get the dictionnary with information on the schema (when schemas catagalogue list already loaded)\n",
    "def get_schema_dict(schema_name, schemas_catalogue_list) :\n",
    "    res = None\n",
    "    for schema in schemas_catalogue_list :\n",
    "        if schema['name'] == schema_name :\n",
    "            res = schema\n",
    "    \n",
    "    if res is None :\n",
    "        print(\"No schema named '{}' found.\".format(schema_name))\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "#Based on validation columns by version, adds a column to the ref_table that shows the most recent version of the schema for which the resource is valid\n",
    "def add_most_recent_valid_version(df_ref) :\n",
    "    \n",
    "    version_cols_list = [col for col in df_ref.columns if col.startswith('is_valid_v_')]\n",
    "    \n",
    "    df_ref['most_recent_valid_version'] = ''\n",
    "    \n",
    "    for col in sorted(version_cols_list, reverse=True) :\n",
    "        df_ref.loc[(df_ref['most_recent_valid_version'] == ''), 'most_recent_valid_version'] = df_ref.loc[(df_ref['most_recent_valid_version'] == ''), col].apply(lambda x : x*col.replace('is_valid_v_',''))\n",
    "    \n",
    "    df_ref.loc[(df_ref['most_recent_valid_version'] == ''), 'most_recent_valid_version'] = np.nan\n",
    "    \n",
    "    return df_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "#Creates a dataset on data.gouv.fr for consolidation files (used only if does not exist yet in config file)\n",
    "def create_schema_consolidation_dataset(schema_name, schemas_catalogue_list, api_url) :\n",
    "    global HEADERS, datasets_description_template, datasets_title_template\n",
    "    \n",
    "    schema_title = get_schema_dict(schema_name, schemas_catalogue_list)['title']\n",
    "    \n",
    "    response = requests.post(api_url + 'datasets/', json={\n",
    "    'title': datasets_title_template.format(schema_title=schema_title),\n",
    "    'description': datasets_description_template.format(schema_name=schema_name),\n",
    "    'organization':'534fff75a3a7292c64a77de4',\n",
    "    'license':'lov2'\n",
    "    }, headers=HEADERS)\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "#Generic function to update a field (key) in the config file\n",
    "def update_config_file(schema_name, key, value, config_path) :\n",
    "    with open(config_path, 'r') as f :\n",
    "        config_dict = yaml.safe_load(f)\n",
    "    \n",
    "    config_dict[schema_name][key] = value\n",
    "    \n",
    "    with open(config_path, 'w') as outfile:\n",
    "        yaml.dump(config_dict, outfile, default_flow_style=False)\n",
    "\n",
    "\n",
    "#Adds the resource ID of the consolidated file for a given schema version in the config file\n",
    "def update_config_version_resource_id(schema_name, version_name, r_id, config_path) :\n",
    "    with open(config_path, 'r') as f :\n",
    "        config_dict = yaml.safe_load(f)\n",
    "    \n",
    "    if 'latest_resource_ids' not in config_dict[schema_name] :\n",
    "        config_dict[schema_name]['latest_resource_ids'] = {version_name: r_id}\n",
    "    else :\n",
    "        config_dict[schema_name]['latest_resource_ids'][version_name] = r_id\n",
    "        \n",
    "    with open(config_path, 'w') as outfile:\n",
    "        yaml.dump(config_dict, outfile, default_flow_style=False)\n",
    "\n",
    "\n",
    "#Returns if resource schema (version) metadata should be updated or not based on what we know about the resource\n",
    "def is_schema_version_to_update(row) :\n",
    "    initial_version_name = row['initial_version_name']\n",
    "    most_recent_valid_version = row['most_recent_valid_version']\n",
    "    resource_found_by = row['resource_found_by']\n",
    "    \n",
    "    return (resource_found_by == '1 - schema request') and (most_recent_valid_version == most_recent_valid_version) and (initial_version_name != most_recent_valid_version)\n",
    "\n",
    "#Returns if resource schema (version) metadata should be added or not based on what we know about the resource\n",
    "def is_schema_to_add(row) :\n",
    "    resource_found_by = row['resource_found_by']\n",
    "    is_valid_one_version = row['is_valid_one_version']\n",
    "    \n",
    "    return (resource_found_by != '1 - schema request') and is_valid_one_version\n",
    "\n",
    "#Returns if resource schema (version) metadata should be deleted or not based on what we know about the resource\n",
    "def is_schema_to_drop(row) :\n",
    "    resource_found_by = row['resource_found_by']\n",
    "    is_valid_one_version = row['is_valid_one_version']\n",
    "    \n",
    "    return (resource_found_by == '1 - schema request') and (is_valid_one_version == False)\n",
    "\n",
    "\n",
    "#Function that adds a schema (version) metadata on a resource\n",
    "def add_resource_schema(api_url, dataset_id, resource_id, schema_name, version_name) :\n",
    "    \n",
    "    global HEADERS\n",
    "    \n",
    "    schema = {\n",
    "        \"name\": schema_name,\n",
    "        \"version\": version_name\n",
    "    }\n",
    "    \n",
    "    try :\n",
    "        url = api_url + 'datasets/{}/resources/{}/'.format(dataset_id, resource_id)\n",
    "        r = requests.get(url, headers=HEADERS)\n",
    "        extras = r.json()['extras']\n",
    "    except :\n",
    "        extras = {}\n",
    "    \n",
    "    extras['consolidation_schema:add_schema'] = schema_name\n",
    "    \n",
    "    obj = {'schema': schema, 'extras': extras}\n",
    "                \n",
    "    url = api_url + 'datasets/{}/resources/{}/'.format(dataset_id, resource_id)\n",
    "    response = requests.put(url, json=obj, headers=HEADERS)\n",
    "    \n",
    "    if response.status_code != 200 :\n",
    "        print('🔴 Schema could not be added on resource. Dataset ID: {} - Resource ID: {}'.format(dataset_id, resource_id))\n",
    "        \n",
    "    return response.status_code == 200\n",
    "\n",
    "\n",
    "#Function that updates a schema (version) metadata on a resource\n",
    "def update_resource_schema(api_url, dataset_id, resource_id, schema_name, version_name) :\n",
    "    \n",
    "    global HEADERS\n",
    "    \n",
    "    schema = {\n",
    "        \"name\": schema_name,\n",
    "        \"version\": version_name\n",
    "    }\n",
    "    \n",
    "    try :\n",
    "        url = api_url + 'datasets/{}/resources/{}/'.format(dataset_id, resource_id)\n",
    "        r = requests.get(url, headers=HEADERS)\n",
    "        extras = r.json()['extras']\n",
    "    except :\n",
    "        extras = {}\n",
    "    \n",
    "    extras['consolidation_schema:update_schema'] = schema_name\n",
    "    \n",
    "    obj = {'schema': schema, 'extras': extras}\n",
    "                \n",
    "    url = api_url + 'datasets/{}/resources/{}/'.format(dataset_id, resource_id)\n",
    "    response = requests.put(url, json=obj, headers=HEADERS)\n",
    "    \n",
    "    if response.status_code != 200 :\n",
    "        print('🔴 Resource schema could not be updated. Dataset ID: {} - Resource ID: {}'.format(dataset_id, resource_id))\n",
    "        \n",
    "    return response.status_code == 200\n",
    "\n",
    "\n",
    "#Function that deletes a schema (version) metadata on a resource\n",
    "def delete_resource_schema(api_url, dataset_id, resource_id, initial_schema_name) :\n",
    "    \n",
    "    global HEADERS\n",
    "    \n",
    "    schema = {}\n",
    "    \n",
    "    try :\n",
    "        url = api_url + 'datasets/{}/resources/{}/'.format(dataset_id, resource_id)\n",
    "        r = requests.get(url, headers=HEADERS)\n",
    "        extras = r.json()['extras']\n",
    "    except :\n",
    "        extras = {}\n",
    "    \n",
    "    extras['consolidation_schema:remove_schema'] = schema_name\n",
    "    \n",
    "    obj = {'schema': schema, 'extras': extras}\n",
    "                \n",
    "    url = api_url + 'datasets/{}/resources/{}/'.format(dataset_id, resource_id)\n",
    "    response = requests.put(url, json=obj, headers=HEADERS)\n",
    "    \n",
    "    if response.status_code != 200 :\n",
    "        print('🔴 Resource schema could not be deleted. Dataset ID: {} - Resource ID: {}'.format(dataset_id, resource_id))\n",
    "    \n",
    "    return response.status_code == 200\n",
    "\n",
    "\n",
    "#Get the (list of) e-mail address(es) of the owner or of the admin(s) of the owner organization of a dataset\n",
    "def get_owner_or_admin_mails(dataset_id, api_url) :\n",
    "    r = requests.get(api_url + 'datasets/{}/'.format(dataset_id))\n",
    "    r_dict = r.json()\n",
    "    \n",
    "    if r_dict['organization'] is not None :\n",
    "        org_id = r_dict['organization']['id']\n",
    "    else :\n",
    "        org_id = None\n",
    "    \n",
    "    if r_dict['owner'] is not None :\n",
    "        owner_id = r_dict['owner']['id']\n",
    "    else :\n",
    "        owner_id = None\n",
    "    \n",
    "    mails_type = None\n",
    "    mails_list = []\n",
    "    \n",
    "    if org_id is not None :\n",
    "        mails_type = 'organisation_admins'\n",
    "        r_org = requests.get(api_url + 'organizations/{}/'.format(org_id))\n",
    "        members_list = r_org.json()['members']\n",
    "        for member in members_list :\n",
    "            if member['role'] == 'admin' :\n",
    "                user_id = member['user']['id']\n",
    "                r_user = requests.get(api_url + 'users/{}/'.format(user_id), headers=HEADER)\n",
    "                user_mail = r_user.json()['email']\n",
    "                mails_list += [user_mail]\n",
    "                \n",
    "    else :\n",
    "        if owner_id is not None :\n",
    "            mails_type = 'owner'\n",
    "            r_user = requests.get(api_url + 'users/{}/'.format(owner_id), headers=HEADER)\n",
    "            user_mail = r_user.json()['email']\n",
    "            mails_list += [user_mail]\n",
    "    \n",
    "    return (mails_type, mails_list)\n",
    "\n",
    "\n",
    "#Function to send a e-mail\n",
    "def send_email(subject, message, mail_from, mail_to, smtp_host, smtp_user, smtp_password):\n",
    "    message = emails.html(html='<p>%s</p>' % message,\n",
    "                        subject=subject,\n",
    "                        mail_from=mail_from)\n",
    "    smtp = {\n",
    "        'host': smtp_host,\n",
    "        'port': 587,\n",
    "        'tls': True,\n",
    "        'user': smtp_user,\n",
    "        'password': smtp_password,\n",
    "    }\n",
    "\n",
    "    _ = message.send(to=mail_to, smtp=smtp)\n",
    "    \n",
    "    return _\n",
    "\n",
    "\n",
    "#Function to post a comment on a dataset\n",
    "def post_comment_on_dataset(dataset_id, title, comment, api_url):\n",
    "    \n",
    "    global HEADER\n",
    "    \n",
    "    post_object = {\n",
    "        'title':title,\n",
    "        'comment' : comment,\n",
    "        'subject': {'class': 'Dataset', 'id': dataset_id}\n",
    "    }\n",
    "    \n",
    "    _ = requests.post(api_url + 'discussions/', json=post_object, headers=HEADER)\n",
    "    \n",
    "    return _\n",
    "\n",
    "\n",
    "def add_validation_extras(dataset_id, resource_id, validation_report_path):\n",
    "    if os.path.isfile(validata_report_path) :\n",
    "\n",
    "        with open(validation_report_path) as out:\n",
    "            validation_report = json.load(out)\n",
    "\n",
    "        global HEADERS\n",
    "        \n",
    "        try :\n",
    "            url = api_url + 'datasets/{}/resources/{}/'.format(dataset_id, resource_id)\n",
    "            r = requests.get(url, headers=HEADERS)\n",
    "            extras = r.json()['extras']\n",
    "        except :\n",
    "            extras = {}\n",
    "\n",
    "        extras = {**extras, **validation_report}\n",
    "        \n",
    "        obj = {'extras': extras}\n",
    "                    \n",
    "        url = api_url + 'datasets/{}/resources/{}/'.format(dataset_id, resource_id)\n",
    "        response = requests.put(url, json=obj, headers=HEADERS)\n",
    "        \n",
    "        if response.status_code != 200 :\n",
    "            print('🔴 Schema could not be added on resource. Dataset ID: {} - Resource ID: {}'.format(dataset_id, resource_id))\n",
    "            \n",
    "        return response.status_code == 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for schema_name in config_dict.keys() :\n",
    "    \n",
    "    with open(config_path, 'r') as f :\n",
    "        config_dict = yaml.safe_load(f)\n",
    "    \n",
    "    print('{} - ℹ️ STARTING SCHEMA: {}'.format(datetime.now(), schema_name))\n",
    "    \n",
    "    schema_consolidated_data_path = Path(consolidated_data_path) / schema_name.replace('/','_')\n",
    "    \n",
    "    if os.path.exists(schema_consolidated_data_path) :\n",
    "        #Check if dataset_id is in config. If not, create a dataset on datagouv\n",
    "        schema_config = config_dict[schema_name]\n",
    "        if(('publication' in schema_config.keys()) and schema_config['publication'] == True): \n",
    "            if 'consolidated_dataset_id' not in schema_config.keys() :\n",
    "                response = create_schema_consolidation_dataset(schema_name, schemas_catalogue_list, api_url)\n",
    "                if response.status_code == 201 :\n",
    "                    consolidated_dataset_id = response.json()['id']\n",
    "                    update_config_file(schema_name, 'consolidated_dataset_id', consolidated_dataset_id, config_path)\n",
    "                    print('{} -- 🟢 No consolidation dataset for this schema - Successfully created (id: {})'.format(datetime.today(), consolidated_dataset_id))\n",
    "                else :\n",
    "                    print('{} -- 🔴 No consolidation dataset for this schema - Failed to create one'.format(datetime.today()))\n",
    "            else :\n",
    "                consolidated_dataset_id = schema_config['consolidated_dataset_id']\n",
    "                \n",
    "            schemas_report_dict[schema_name]['consolidated_dataset_id'] = consolidated_dataset_id\n",
    "                \n",
    "            #Creating last consolidation resources\n",
    "            version_names_list = [filename.replace('consolidation_'+schema_name.replace('/','_')+'_v_', '').replace('_'+ consolidation_date_str +'.csv','') for filename in os.listdir(schema_consolidated_data_path) if not filename.startswith('.')]\n",
    "            \n",
    "            for version_name in sorted(version_names_list) :\n",
    "                with open(config_path, 'r') as f :\n",
    "                    config_dict = yaml.safe_load(f)\n",
    "            \n",
    "                schema = {\n",
    "                    \"name\": schema_name,\n",
    "                    \"version\": version_name\n",
    "                }\n",
    "                obj = {}\n",
    "                obj['schema'] = schema\n",
    "                obj['type'] = 'main'\n",
    "                obj['title'] = \"Dernière version consolidée (v{} du schéma) - {}\".format(version_name, consolidation_date_str)\n",
    "                obj['format'] = 'csv'\n",
    "                \n",
    "                file_path = os.path.join(schema_consolidated_data_path, 'consolidation_{}_v_{}_{}.csv'.format(schema_name.replace('/','_'), version_name, consolidation_date_str))\n",
    "                \n",
    "                \n",
    "                #Uploading file (creating a new resource if version was not there before)\n",
    "                try :\n",
    "                    r_id = config_dict[schema_name]['latest_resource_ids'][version_name]\n",
    "                    url = api_url + 'datasets/' + consolidated_dataset_id + '/resources/' + r_id + '/upload/'\n",
    "                    r_to_create = False\n",
    "                    expected_status_code = 200\n",
    "                    \n",
    "                except KeyError :\n",
    "                    url = api_url + 'datasets/' + consolidated_dataset_id + '/upload/'\n",
    "                    r_to_create = True\n",
    "                    expected_status_code = 201\n",
    "                \n",
    "                with open(file_path, 'rb') as file:\n",
    "                    files = {'file': (file_path.split('/')[-1], file.read())}\n",
    "                \n",
    "                response = requests.post(url, files=files, headers=HEADERS)\n",
    "\n",
    "                if response.status_code == expected_status_code :\n",
    "                    if r_to_create == True :\n",
    "                        r_id = response.json()['id']\n",
    "                        update_config_version_resource_id(schema_name, version_name, r_id, config_path)\n",
    "                        print('{} --- ➕ New latest resource ID created for {} v{} (id: {})'.format(datetime.today(), schema_name, version_name, r_id))\n",
    "                else :\n",
    "                    r_id = None\n",
    "                    print('{} --- ⚠️ Version {}: file could not be uploaded.'.format(datetime.today(), version_name))\n",
    "                    \n",
    "                    \n",
    "                if r_id is not None :\n",
    "                    r_url = api_url + 'datasets/{}/resources/{}/'.format(consolidated_dataset_id, r_id)\n",
    "                    r_response = requests.put(r_url, json=obj, headers=HEADERS)\n",
    "\n",
    "                    if r_response.status_code == 200 :\n",
    "                        if r_to_create == True :\n",
    "                            print('{} --- ✅ Version {}: Successfully created consolidated file.'.format(datetime.today(), version_name))\n",
    "                        else :\n",
    "                            print('{} --- ✅ Version {}: Successfully updated consolidated file.'.format(datetime.today(), version_name))\n",
    "                    else :\n",
    "                        print('{} --- ⚠️ Version {}: file uploaded but metadata could not be updated.'.format(datetime.today(), version_name))\n",
    "        else:\n",
    "            schemas_report_dict[schema_name]['consolidated_dataset_id'] = np.nan\n",
    "            print('{} -- ❌ No publication for this schema.'.format(datetime.today()))\n",
    "\n",
    "    else :\n",
    "        schemas_report_dict[schema_name]['consolidated_dataset_id'] = np.nan\n",
    "        print('{} -- ❌ No consolidated file for this schema.'.format(datetime.today()))\n",
    "\n",
    "#Reopening config file to update config_dict (in case it has to be reused right after)\n",
    "with open(config_path, 'r') as f :\n",
    "    config_dict = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas (versions) feedback loop on resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding needed infos for each resource in reference tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for schema_name in config_dict.keys() :\n",
    "    \n",
    "    ref_table_path = os.path.join(ref_tables_path, 'ref_table_{}.csv'.format(schema_name.replace('/', '_')))\n",
    "    \n",
    "    if os.path.isfile(ref_table_path) :\n",
    "        df_ref = pd.read_csv(ref_table_path)\n",
    "\n",
    "        df_ref = add_most_recent_valid_version(df_ref)\n",
    "        df_ref['is_schema_version_to_update'] = df_ref.apply(is_schema_version_to_update, axis=1)\n",
    "        df_ref['is_schema_to_add'] = df_ref.apply(is_schema_to_add, axis=1)\n",
    "        df_ref['is_schema_to_drop'] = df_ref.apply(is_schema_to_drop, axis=1)\n",
    "        \n",
    "        df_ref.to_csv(ref_table_path, index=False)\n",
    "\n",
    "        print('{} - ✅ Infos added for schema {}'.format(datetime.today(), schema_name))\n",
    "        \n",
    "    else :\n",
    "        print('{} - ❌ No reference table for schema {}'.format(datetime.today(), schema_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating resources schemas and sending comments/mails to notify producers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️⚠️⚠️ **TODO: UNCOMMENT MAIL SENDING AND DISCUSSION COMMENTING (+ DELETE PRINTS) FOR NOTIFICATION TO PRODUCERS.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for schema_name in config_dict.keys() :\n",
    "    \n",
    "    ref_table_path = os.path.join(ref_tables_path, 'ref_table_{}.csv'.format(schema_name.replace('/', '_')))\n",
    "    \n",
    "    if os.path.isfile(ref_table_path) :\n",
    "        df_ref = pd.read_csv(ref_table_path)\n",
    "        df_ref['resource_schema_update_success'] = np.nan\n",
    "        df_ref['producer_notification_success'] = np.nan\n",
    "\n",
    "        for idx, row in df_ref.iterrows() :\n",
    "            if row['is_schema_version_to_update'] :\n",
    "                resource_update_success = update_resource_schema(api_url, row['dataset_id'], row['resource_id'], schema_name, row['most_recent_valid_version'])\n",
    "                df_ref.loc[(df_ref['resource_id'] == row['resource_id']), 'resource_schema_update_success'] = resource_update_success\n",
    "                \n",
    "                if resource_update_success == True :\n",
    "                    title = 'Mise à jour de la version de la métadonnée schéma'\n",
    "                    comment = updated_schema_comment_template.format(resource_title = row['resource_title'],\n",
    "                                                                     schema_name = schema_name,\n",
    "                                                                     initial_version_name = row['initial_version_name'],\n",
    "                                                                     most_recent_valid_version = row['most_recent_valid_version']\n",
    "                                                                    )\n",
    "                    #comment_post = post_comment_on_dataset(dataset_id=row['dataset_id'],\n",
    "                    #                                       title=title,\n",
    "                    #                                       comment=comment,\n",
    "                    #                                       api_url=api_url\n",
    "                    #                                      )\n",
    "                    #\n",
    "                    #producer_notification_success = (comment_post.status_code == 201)\n",
    "                    \n",
    "                    #df_ref.loc[(df_ref['resource_id'] == row['resource_id']), 'producer_notification_success'] = producer_notification_success\n",
    "                    #No notification at the moment:\n",
    "                    df_ref.loc[(df_ref['resource_id'] == row['resource_id']), 'producer_notification_success'] = False\n",
    "                    \n",
    "            \n",
    "            elif row['is_schema_to_add'] :\n",
    "                resource_update_success = add_resource_schema(api_url, row['dataset_id'], row['resource_id'], schema_name, row['most_recent_valid_version'])\n",
    "                df_ref.loc[(df_ref['resource_id'] == row['resource_id']), 'resource_schema_update_success'] = resource_update_success\n",
    "                \n",
    "                if resource_update_success == True :\n",
    "                    title = 'Ajout de la métadonnée schéma'\n",
    "                    comment = added_schema_comment_template.format(resource_title = row['resource_title'],\n",
    "                                                             schema_name = schema_name,\n",
    "                                                             most_recent_valid_version = row['most_recent_valid_version']\n",
    "                                                            )\n",
    "                    #comment_post = post_comment_on_dataset(dataset_id=row['dataset_id'],\n",
    "                    #                                       title=title,\n",
    "                    #                                       comment=comment,\n",
    "                    #                                       api_url=api_url\n",
    "                    #                                      )\n",
    "                    #\n",
    "                    #producer_notification_success = (comment_post.status_code == 201)\n",
    "                    #df_ref.loc[(df_ref['resource_id'] == row['resource_id']), 'producer_notification_success'] = producer_notification_success\n",
    "                    #No notification at the moment:\n",
    "                    df_ref.loc[(df_ref['resource_id'] == row['resource_id']), 'producer_notification_success'] = False\n",
    "            \n",
    "            #Right now, we don't drop schema and do no notification\n",
    "            elif row['is_schema_to_drop'] :\n",
    "            #    resource_update_success = delete_resource_schema(api_url, row['dataset_id'], row['resource_id'], schema_name)\n",
    "            #    df_ref.loc[(df_ref['resource_id'] == row['resource_id']), 'resource_schema_update_success'] = resource_update_success\n",
    "            #    \n",
    "            #    if resource_update_success == True :\n",
    "            #        title = 'Suppression de la métadonnée schéma'\n",
    "            #        \n",
    "            #        mails_type, mails_list = get_owner_or_admin_mails(row['dataset_id'], api_url)\n",
    "            #        \n",
    "            #        if len(mails_list) > 0 : #If we found some email addresses, we send mails\n",
    "            #            \n",
    "            #            if mails_type == 'organisation_admins' :\n",
    "            #                message = deleted_schema_mail_template_org.format(organisation_name=row['organization_or_owner'],\n",
    "            #                                                                  dataset_title=row['dataset_title'],\n",
    "            #                                                                  resource_title=row['resource_title'],\n",
    "            #                                                                  schema_name=schema_name,\n",
    "            #                                                                  schema_url=get_schema_dict(schema_name, schemas_catalogue_list)['schema_url'],\n",
    "            #                                                                  resource_url=row['resource_url']\n",
    "            #                                                                 )\n",
    "            #            elif mails_type == 'owner' :\n",
    "            #                message = deleted_schema_mail_template_own.format(dataset_title=row['dataset_title'],\n",
    "            #                                                                  resource_title=row['resource_title'],\n",
    "            #                                                                  schema_name=schema_name,\n",
    "            #                                                                  schema_url=get_schema_dict(schema_name, schemas_catalogue_list)['schema_url'],\n",
    "            #                                                                  resource_url=row['resource_url']\n",
    "            #                                                                 )\n",
    "            #                \n",
    "            #            \n",
    "            #            #Sending mail\n",
    "            #            \n",
    "            #            producer_notification_success_list = []\n",
    "            #            print('- {} | {}:'.format(row['dataset_title'], row['resource_title']))\n",
    "            #            for mail_to in mails_list :\n",
    "            #                #mail_send = send_email(subject=title,\n",
    "            #                #                       message=message,\n",
    "            #                #                       mail_from=mail_from,\n",
    "            #                #                       mail_to=mail_to,\n",
    "            #                #                       smtp_host=smtp_host,\n",
    "            #                #                       smtp_user=smtp_user,\n",
    "            #                #                       smtp_password=smtp_password)\n",
    "\n",
    "            #                #producer_notification_success_list += [(mail_send.status_code == 250)]\n",
    "            #            \n",
    "            #            #producer_notification_success = any(producer_notification_success_list) # Success if at least one person receives the mail\n",
    "            #            \n",
    "            #        else : #If no mail address, we post a comment on dataset\n",
    "            #            comment = deleted_schema_comment_template.format(resource_title=row['resource_title'],\n",
    "            #                                                             schema_name=schema_name,\n",
    "            #                                                             schema_url=get_schema_dict(schema_name, schemas_catalogue_list)['schema_url'],\n",
    "            #                                                             resource_url=row['resource_url']\n",
    "            #                                                            )\n",
    "            #            \n",
    "            #            #comment_post = post_comment_on_dataset(dataset_id=row['dataset_id'],\n",
    "            #            #                                       title=title,\n",
    "            #            #                                       comment=comment,\n",
    "            #            #                                       api_url=api_url\n",
    "            #            #                                      )\n",
    "            #        \n",
    "            #            #producer_notification_success = (comment_post.status_code == 201)\n",
    "            #        \n",
    "            #        #df_ref.loc[(df_ref['resource_id'] == row['resource_id']), 'producer_notification_success'] = producer_notification_success\n",
    "            \n",
    "                #TO DROP when schema will be deleted and producer notified:\n",
    "                df_ref.loc[(df_ref['resource_id'] == row['resource_id']), 'resource_schema_update_success'] = False\n",
    "                df_ref.loc[(df_ref['resource_id'] == row['resource_id']), 'producer_notification_success'] = False\n",
    "            \n",
    "        \n",
    "        df_ref.to_csv(ref_table_path, index=False)\n",
    "\n",
    "        print('{} - ✅ Resources updated for schema {}'.format(datetime.today(), schema_name))\n",
    "        \n",
    "    else :\n",
    "        print('{} - ❌ No reference table for schema {}'.format(datetime.today(), schema_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add validata report to extras for each resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for schema_name in config_dict.keys() :\n",
    "    \n",
    "    ref_table_path = os.path.join(ref_tables_path, 'ref_table_{}.csv'.format(schema_name.replace('/', '_')))\n",
    "    \n",
    "    if os.path.isfile(ref_table_path) :\n",
    "        df_ref = pd.read_csv(ref_table_path)\n",
    "        df_ref['resource_schema_update_success'] = np.nan\n",
    "        df_ref['producer_notification_success'] = np.nan\n",
    "\n",
    "        for idx, row in df_ref.iterrows() :\n",
    "            validata_report_path = str(validata_reports_path) + '/' + row['dataset_id'] + '_' + row['resource_id'] + '_'\n",
    "\n",
    "            # If there is a valid version, put validata report from it\n",
    "            if(row['most_recent_valid_version'] == row['most_recent_valid_version']):\n",
    "                validata_report_path +=  row['most_recent_valid_version'] + '.json'\n",
    "            # Else, check if declarative version\n",
    "            else:\n",
    "                # If so, put validation report from it\n",
    "                if(row['initial_version_name'] == row['initial_version_name']):\n",
    "                    validata_report_path += row['initial_version_name'] + '.json'\n",
    "                # If not, put validation report from latest version\n",
    "                else:\n",
    "                   validata_report_path += max([x.replace('is_valid_v_','') for x in list(row.keys()) if 'is_valid_v_' in x]) + '.json'\n",
    "            \n",
    "            add_validation_extras(row['dataset_id'], row['resource_id'], validata_report_path)            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating consolidation documentation resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for schema_name in config_dict.keys() :\n",
    "    \n",
    "    ref_table_path = os.path.join(ref_tables_path, 'ref_table_{}.csv'.format(schema_name.replace('/', '_')))\n",
    "    \n",
    "    with open(config_path, 'r') as f :\n",
    "        config_dict = yaml.safe_load(f)\n",
    "    \n",
    "    print('{} - ℹ️ STARTING SCHEMA: {}'.format(datetime.now(), schema_name))\n",
    "    \n",
    "    schema_config = config_dict[schema_name]\n",
    "    if(('publication' in schema_config.keys()) and schema_config['publication'] == True): \n",
    "        if os.path.isfile(ref_table_path) :\n",
    "        \n",
    "            if 'consolidated_dataset_id' in schema_config.keys() :\n",
    "                consolidated_dataset_id = schema_config['consolidated_dataset_id']\n",
    "                \n",
    "                obj = {}\n",
    "                obj['type'] = 'documentation'\n",
    "                obj['title'] = \"Documentation sur la consolidation - {}\".format(consolidation_date_str)\n",
    "\n",
    "                #Uploading documentation file (creating a new resource if version was not there before)\n",
    "                try :\n",
    "                    doc_r_id = config_dict[schema_name]['documentation_resource_id']\n",
    "                    url = api_url + 'datasets/' + consolidated_dataset_id + '/resources/' + doc_r_id + '/upload/'\n",
    "                    doc_r_to_create = False\n",
    "                    expected_status_code = 200\n",
    "\n",
    "                except KeyError :\n",
    "                    url = api_url + 'datasets/' + consolidated_dataset_id + '/upload/'\n",
    "                    doc_r_to_create = True\n",
    "                    expected_status_code = 201\n",
    "\n",
    "                with open(ref_table_path, 'rb') as file:\n",
    "                    files = {'file': (ref_table_path.split('/')[-1], file.read())}\n",
    "\n",
    "                response = requests.post(url, files=files, headers=HEADERS)\n",
    "                \n",
    "                if response.status_code == expected_status_code :\n",
    "                    if doc_r_to_create == True :\n",
    "                        doc_r_id = response.json()['id']\n",
    "                        update_config_file(schema_name, 'documentation_resource_id', doc_r_id, config_path)\n",
    "                        print('{} --- ➕ New documentation resource ID created for {} (id: {})'.format(datetime.today(), schema_name, doc_r_id))\n",
    "                else :\n",
    "                    doc_r_id = None\n",
    "                    print('{} --- ⚠️ Documentation file could not be uploaded.'.format(datetime.today()))\n",
    "\n",
    "\n",
    "                if doc_r_id is not None :\n",
    "                    doc_r_url = api_url + 'datasets/{}/resources/{}/'.format(consolidated_dataset_id, doc_r_id)\n",
    "                    doc_r_response = requests.put(doc_r_url, json=obj, headers=HEADERS)\n",
    "                    if doc_r_response.status_code == 200 :\n",
    "                        if doc_r_to_create == True :\n",
    "                            print('{} --- ✅ Successfully created documentation file.'.format(datetime.today()))\n",
    "                        else :\n",
    "                            print('{} --- ✅ Successfully updated documentation file.'.format(datetime.today()))\n",
    "                    else :\n",
    "                        print('{} --- ⚠️ Documentation file uploaded but metadata could not be updated.'.format(datetime.today()))\n",
    "            \n",
    "            else :\n",
    "                print('{} -- ❌ No consolidation dataset ID for this schema.'.format(datetime.today()))\n",
    "                \n",
    "        else :\n",
    "            print('{} -- ❌ No reference table for this schema.'.format(datetime.today()))\n",
    "    \n",
    "    else :\n",
    "        print('{} -- ❌ No publication for this schema.'.format(datetime.today()))\n",
    "\n",
    "#Reopening config file to update config_dict (in case it has to be reused right after)\n",
    "with open(config_path, 'r') as f :\n",
    "    config_dict = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidation Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report by schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_list = []\n",
    "\n",
    "for schema_name in schemas_report_dict.keys() :\n",
    "    schema_report_dict = schemas_report_dict[schema_name]\n",
    "    schema_report_dict['schema_name'] = schema_name\n",
    "    reports_list += [schema_report_dict]\n",
    "    \n",
    "reports_df = pd.DataFrame(reports_list)\n",
    "\n",
    "reports_df = reports_df[['schema_name'] + [col for col in reports_df.columns if col != 'schema_name']].rename(columns={'config_created':'new_config_created'}) #rename to drop at next launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df_list = []\n",
    "for schema_name in config_dict.keys() :\n",
    "    \n",
    "    ref_table_path = os.path.join(ref_tables_path, 'ref_table_{}.csv'.format(schema_name.replace('/', '_')))\n",
    "    \n",
    "    if os.path.isfile(ref_table_path) :\n",
    "        df_ref = pd.read_csv(ref_table_path)\n",
    "        df_ref['schema_name'] = schema_name\n",
    "        df_ref['is_schema_version_updated'] = df_ref['is_schema_version_to_update'] & df_ref['resource_schema_update_success']\n",
    "        df_ref['is_schema_added'] = df_ref['is_schema_to_add'] & df_ref['resource_schema_update_success']\n",
    "        df_ref['is_schema_dropped'] = df_ref['is_schema_to_drop'] & df_ref['resource_schema_update_success']\n",
    "        df_ref['resource_schema_update_success'] = False\n",
    "        df_ref.to_csv(ref_table_path, index=False)\n",
    "        stats_df_list += [df_ref[['schema_name', 'is_schema_version_to_update', 'is_schema_to_add', 'is_schema_to_drop', 'resource_schema_update_success', 'is_schema_version_updated','is_schema_added', 'is_schema_dropped']].fillna(False).groupby('schema_name').sum().reset_index()]\n",
    "\n",
    "stats_df = pd.concat(stats_df_list).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_df = reports_df.merge(stats_df, on='schema_name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_df.to_excel(os.path.join(report_tables_path, 'report_by_schema_{}.xlsx'.format(consolidation_date_str)), index=False)\n",
    "reports_df.to_csv(os.path.join(report_tables_path, 'report_by_schema_{}.csv'.format(consolidation_date_str)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed reports (by schema and resource source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for schema_name in config_dict.keys() :\n",
    "    \n",
    "    ref_table_path = os.path.join(ref_tables_path, 'ref_table_{}.csv'.format(schema_name.replace('/', '_')))\n",
    "    \n",
    "    if os.path.isfile(ref_table_path) :\n",
    "        df_ref = pd.read_csv(ref_table_path)        \n",
    "    \n",
    "        df_ref['total_nb_resources'] = 1\n",
    "        df_ref['error_type'].fillna('no-error', inplace=True)\n",
    "\n",
    "        cols_to_sum = ['total_nb_resources']\n",
    "        cols_to_sum += [col for col in df_ref.columns if col.startswith('is_')]\n",
    "        df_report = df_ref.groupby(['resource_found_by', 'error_type']).agg({col:sum for col in cols_to_sum}).reset_index()\n",
    "\n",
    "        df_report.to_excel(os.path.join(report_tables_path, 'report_table_{}.xlsx'.format(schema_name.replace('/', '_'))), index=False)\n",
    "\n",
    "        print('{} - ✅ Report done for schema {}'.format(datetime.today(), schema_name))\n",
    "        \n",
    "    else :\n",
    "        print('{} - ❌ No reference table for schema {}'.format(datetime.today(), schema_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.move(TMP_FOLDER + 'consolidated_data', OUTPUT_DATA_FOLDER)\n",
    "shutil.move(TMP_FOLDER + 'ref_tables', OUTPUT_DATA_FOLDER)\n",
    "shutil.move(TMP_FOLDER + 'report_tables', OUTPUT_DATA_FOLDER)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
